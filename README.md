# Two_Link_System
Designing a controller under the assumption that the structure is
completely unknown for π!(β…, β…Μ‡ +\
From the dynamics,\
π‘€(β…)β…Μ + π¶(β…, β…Μ‡ + + πΊ(β…) + π!(β…, β…Μ‡ + = π\
π‘€(β…)β…Μ + π¶(β…, β…Μ‡ + + πΊ(β…) accounts to the structured uncertainties and
π!(β…, β…Μ‡ + represents the unstructured uncertainties. And as a result, we
can design a linear in the parameter design for the structured
uncertainties. And the unstructured uncertainties can be estimated using
deep neural network.

For deep neural network π!(β…, β…Μ‡ + can be estimated as\
π!(β…, β…Μ‡ + = π‘\*π(Ξ¦(β…, β…Μ‡ + + π€(β…, β…Μ‡ +\
Where π‘π– β„+Γ—\#is the output weights, π(. )π– β„+ is the activation
function, Ξ¦is the inner and the reconstruction error is π€(β…, β…Μ‡ + π– β„\#.

Ξ¦\],- = π,(π‘‰\_,-\*π™\_,.\" - + π‘\_,-)\
Error Dynamics-\
π‘’ = β…! β’ β…\
π‘’Μ‡ = β…! β’ β…\
π‘’Μ = β…! β’ β…\
Reference tracking error\
π‘ = π‘’Μ‡ + π›Όπ‘’\
π‘Μ‡ = π‘’Μ + π›Όπ‘’Μ‡\
π‘Μ‡ = β…! β’ β…Μ + π›Όπ‘’Μ‡\
Multiplying both sides by π‘€(β…) we get,\
π‘€(β…)π‘Μ‡ = π‘€(β…)β…! β’ π‘€(β…)β…Μ + π‘€(β…)π›Όπ‘’Μ‡\
From the dynamics we know,\
π‘€(β…)β…Μ = β’π¶(β…, β…Μ‡ + β’ πΊ(β…) β’ π!(β…, β…Μ‡ + + π\
On substitution we get,\
π‘€(β…)π‘Μ‡ = π‘€(β…)β…! β’ π‘€(β…)β…Μ + π‘€(β…)π›Όπ‘’Μ‡\
π‘€(β…)π‘ = π‘€(β…)β…! β’ (β’π¶(β…, β…Μ‡ + β’ πΊ(β…) β’ π!(β…, β…Μ‡ + + π) + π‘€(β…)π›Όπ‘’Μ‡ \"\
\#π‘€Μ‡ (β…, β…Μ‡ +π‘Add and subtract by\
π‘€(β…)π‘Μ‡ = π‘€(β…)β…! + π¶(β…, β…Μ‡ + + πΊ(β…) + π!(β…, β…Μ‡ + β’ π + π‘€(β…)π›Όπ‘’Μ‡ Β± 1~~2~~ π‘€Μ‡ (β…,
β…Μ‡ +π‘ π‘€(β…)π‘Μ‡ = π‘€(β…)(β…! + π›Όπ‘’Μ‡) + π¶(β…, β…Μ‡ + + πΊ(β…) + π!(β…, β…Μ‡ + β’ π Β± 1~~2~~ π‘€Μ‡
(β…, β…Μ‡ +π‘

\"\
We can estimate π‘€(β…) (β…! + π›Όπ‘’Μ‡) + π¶(β…, β…Μ‡ + + πΊ(β…) +\#π‘€Μ‡ (β…, β…Μ‡ +π‘ = π‘πƒ
since it is linear in the unknown parameters and we were able to develop
the unknown parameters πƒ as follows, \# + π‘\#π‘™\"\# + π‘\#π‘™\#\#β΅πƒ\" βΆ βΆ βΆ
πƒ\# πƒ0 πƒ( β¤ β¥ β¥ β¥ = β΅π‘\"π‘™\" βΆ βΆ βΆ βΆ (π‘\" + π‘\#)π‘™\" π‘\#π‘™\"π‘™\# π‘\#π‘™\" \# β¤
β¥ β¥ β¥ β¥\
β£πƒ1β¦ β£ π‘\#π‘™\# β¦

And π‘ = π‘2(β…, β…! + π›Όπ‘’Μ‡+ + π‘3(β…, β…Μ‡ + + π‘4(β…) +

> \"

The unstructured state-dependent disturbance π!(β…, β…Μ‡ + will be estimated
using a two-layer neural network.

π!(β…, β…Μ‡ + = π‘\*π(Ξ¦(π) + π€(β…, β…Μ‡ +\
Now,\
π‘€(β…)π‘Μ‡ = π‘πƒ + π‘\*π(Ξ¦(π)) + π€(β…, β…Μ‡ + β’ π β’ 1~~2~~ π‘€Μ‡ (β…, β…Μ‡ +π‘\
Approximation for\
~~π‘‘Ξ¦~~ Ξ¦o + π€6(Ξ¦o \#)π(Ξ¦) = π(Ξ¦\]+ + π‘‘π\
Where,\
Ξ¦o = Ξ¦ β’ Ξ¦\] = π‘‰\*π β’ π‘‰\_ \*π = π‘‰pπ\
π = π + π‘‘π~~π‘‘Ξ¦~~ Ξ¦o + π€6\
Now on substitution we get,\
~~2~~ π‘€Μ‡ (β…, β…Μ‡ +π‘π‘€(β…)π‘Μ‡ = π‘πƒ + π‘\*(π(Ξ¦\]+ + π‘‘π~~π‘‘Ξ¦~~ Ξ¦o + π€6(Ξ¦o \#)) +
π€(β…, β…Μ‡ + β’ π β’ 1\
Add and subtract π‘\] \*πq7Ξ¦o

π‘€(β…)π‘Μ‡ = π‘πƒ + π‘\*rπ(Ξ¦\]+ + π‘‘π~~π‘‘Ξ¦~~ Ξ¦o + π€6(Ξ¦o \#+s + π€(β…, β…Μ‡ + β’ π β’
1~~2~~ π‘€Μ‡ (β…, β…Μ‡ +π‘ Β± π‘\] \*πq7Ξ¦o

π‘€(β…)π‘Μ‡ = π‘πƒ + π‘\*π(Ξ¦\]+ β’ π β’ 1~~2~~ π‘€Μ‡ (β…, β…Μ‡ +π‘ + π‘\] \*πq7Ξ¦o + π‘o
\*πq7Ξ¦o + π‘\*π€6(Ξ¦o \#+ + π€(β…, β…Μ‡ +Let\
π‘o \*πq7Ξ¦o + π‘\*π€6(Ξ¦o \#+ + π€(β…, β…Μ‡ + = π›Ώ\
π‘€(β…)π‘Μ‡ = π‘πƒ + π‘\*π(Ξ¦\]+ β’ π β’ 1~~2~~ π‘€Μ‡ (β…, β…Μ‡ +π‘\
Let the stacked errors\
π‘’\
π‘\
π‰ = v πƒp x π– β„\#8\#818\#+81,\
π‘£π‘’π‘(π‘o )

Where,\
β΅π‘\"\" y β¤\
π‘£π‘’π‘(π‘o + = βΆ βΆ βΆ π‘+\" π‘\"\# y y β‹® β¥ β¥ β¥\
βΆ β‹® β¥\
β£π‘+\# y β¦\
Lyapunov Candidate,\
π‘‰ = 1~~2~~ π‘’\*π‘’ + 1~~2~~ π‘\*π‘€(β…)π‘ + 1~~2~~ πƒp\*Ξ“9.\"πƒp + 1~~2~~ π‘΅π‘(π‘o
\*Ξ“:.\"π‘o ) π‘‰Μ‡ = π‘’\*π‘’ + 1~~2~~ π‘\*π‘€Μ‡ (β…)π‘ + π‘\*π‘€π‘Μ‡ + πƒp\*Ξ“9.\"πƒp + π‘΅π‘(π‘o
\*Ξ“:.\"π‘o +

We know,

> π‘ = π‘’Μ‡ = π‘ β’ π›Όπ‘’\
> πƒΜ‡p = β’πƒΜ‡\_\
> π‘Μ‡o = β’π‘Μ‡\]

And\
π‘€(β…)π‘Μ‡ = π‘πƒ + π‘\*π(Ξ¦\]+ β’ π β’ 1~~2~~ π‘€Μ‡ (β…, β…Μ‡ +π‘ On substitution we get,\
π‘‰Μ‡ = π‘’\*π‘’ + 1~~2~~ π‘\*π‘€Μ‡ (β…)π‘ + π‘\*π‘€π‘Μ‡ + πƒp\*Ξ“9.\"πƒp + π‘΅π‘ :π‘o \*Ξ“:.\"π‘Μ‡o ?

π‘‰Μ‡ = π‘’\*π‘’Μ‡ + 1~~2~~ π‘\*π‘€Μ‡ (β…)π‘ + π‘\*(π‘πƒ + π‘\*π(Ξ¦\]+ β’ π β’ 1~~2~~ π‘€Μ‡ (β…, β…Μ‡
+π‘) + πƒp\*Ξ“9.\"πƒΜ‡p + π‘΅π‘ :π‘o \*Ξ“:.\"π‘Μ‡o ?

On simplification we get,\
.\"π‘Μ‡\] ?π‘‰Μ‡ = π‘’\*(π‘ β’ π›Όπ‘’) + π‘\*(π‘πƒ + π‘\*π(Ξ¦\]+ β’ π + β’ πƒp\*Ξ“9.\"πƒΜ‡\_ β’ π‘΅π‘
:π‘o \*Ξ“:

Design the input π\
π = π‘’ + π‘πƒ\_ + π‘\] \*πq- + π›½;π‘ π‘”π‘›(π‘) + π›½\<π‘\
π‘‰Μ‡ = π‘’\*(π‘ β’ π›Όπ‘’) + π‘\*(π‘πƒ + π‘\*π(Ξ¦\]+ β’ (π‘’ + π‘πƒ\_ + π‘\] \*πq- + π›½;π‘ π‘”π‘›(π‘)
+ π›½\<π‘)+ β’ πƒp\*Ξ“9.\"πƒΜ‡\_ β’ π‘΅π‘ :π‘o \*Ξ“:.\"π‘Μ‡\] ?

π‘‰Μ‡ = β’(π‘’\*π›Όπ‘’) + π‘\*π‘πƒp + π‘\*π‘o \*πq- β’ π‘\*π›½;π‘ π‘”π‘›(π‘) β’ π‘\*π›½\<π‘ β’
πƒp\*Ξ“9.\"πƒΜ‡\_ β’ π‘΅π‘ :π‘o \*Ξ“:.\"π‘Μ‡\] ?

Design for πƒΜ‡\_, we want to get,

So,

Design for π‘Μ‡\], we want to get,

> π‘\*π‘πƒp β’ πƒp\*Ξ“9.\"πƒΜ‡\_ = 0\
> πƒp\*Ξ“9.\"πƒΜ‡\_ = π‘\*π‘πƒp
>
> πƒΜ‡\_ = proj(Ξ“9π‘\*π‘)
>
> π‘\*π‘o \*Οƒβ€ β’ π‘΅π‘ :π‘o \*Ξ“:.\"π‘Μ‡\] ? = 0

We know,\
π‘΅π‘(π‘π‘\*) = π‘\*π‘\
π‘΅π‘(π‘o \*Οƒβ€π‘\*+ = π‘΅π‘(π‘o \*Ξ“:.\"π‘Μ‡\] )So,\
π‘Μ‡\] = π‘π‘π‘π‘—(Ξ“:πq-β€ π‘\*)Yielding\
π‘‰,= β‰¤ β’(π‘’\*π›Όπ‘’) β’ π‘\*π›½\<π‘ =%\>,!?

Using Barbalat's Lemma we can show the β€–π‘’β€–, β€–π‘β€–Λ†β―β―β―Ε  0 which implies
Asymptotic tracking

Simulation results

> 1)The first 4 DNN choices were a good first choice for the
> architectures because they were easy to implement and these function
> restricted the bounds of the data to be in the range of -1 to 1,
> ensuring that the function does not become unstable.
>
> 2)After selecting different activation function for each layer, the
> results showed that linear ReLus with a different output layer give
> the most accurate approximation of the function, so I used multiple
> ReLu with a 2 layer of the Sigmoid at the end for the last design.
>
> 3)Norm of tracking error and filter tracking error From Two-layer
> Neural network
>
> ![](vertopal_84a8f5d296e74188ac300df24ca140fa/media/image1.png){width="5.277777777777778in"
> height="3.7083333333333335in"}
>
> ![](vertopal_84a8f5d296e74188ac300df24ca140fa/media/image2.png){width="5.277777777777778in"
> height="3.7222222222222223in"}
>
> From Gaussian activation functions
>
> ![](vertopal_84a8f5d296e74188ac300df24ca140fa/media/image3.png){width="5.583333333333333in"
> height="3.9722222222222223in"}
>
> For Tanh activation function
>
> ![](vertopal_84a8f5d296e74188ac300df24ca140fa/media/image4.png){width="5.583333333333333in"
> height="4.388888888888889in"}
>
> From Relu with Sigmoid output
>
> ![](vertopal_84a8f5d296e74188ac300df24ca140fa/media/image5.png){width="5.583333333333333in"
> height="4.402777777777778in"}

ReLu with Tanh output

> ![](vertopal_84a8f5d296e74188ac300df24ca140fa/media/image6.png){width="5.583333333333333in"
> height="4.388888888888889in"}

From more ReLu with 2 Sigmoid layers

> ![](vertopal_84a8f5d296e74188ac300df24ca140fa/media/image7.png){width="5.583333333333333in"
> height="4.388888888888889in"}

For the graphs it can be seen that the Relu with sigmoid output
performed the best with 1.5 RMS error compared to the other
architectures. But all the other design performed well in tracking the
objective.

\(4\) Norm of the function approximation

For Gaussian activation

> ![](vertopal_84a8f5d296e74188ac300df24ca140fa/media/image8.png){width="5.638888888888889in"
> height="4.388888888888889in"}

For Tanh activation

![](vertopal_84a8f5d296e74188ac300df24ca140fa/media/image9.png){width="0.18055555555555555in"
height="0.6388888888888888in"}

> ![](vertopal_84a8f5d296e74188ac300df24ca140fa/media/image10.png){width="5.430555555555555in"
> height="4.402777777777778in"}

For ReLu with Sigmoid output

> ![](vertopal_84a8f5d296e74188ac300df24ca140fa/media/image11.png){width="4.513888888888889in"
> height="3.5277777777777777in"}

For ReLu with Tanh output

> ![](vertopal_84a8f5d296e74188ac300df24ca140fa/media/image12.png){width="4.069444444444445in"
> height="3.1805555555555554in"}

Any design

> ![](vertopal_84a8f5d296e74188ac300df24ca140fa/media/image13.png){width="5.055555555555555in"
> height="3.3333333333333335in"}
>
> The last three design seem to perform the best.

\(5\) Input plots

\(a\)

![](vertopal_84a8f5d296e74188ac300df24ca140fa/media/image14.png){width="0.20833333333333334in"
height="0.4305555555555556in"}

> ![](vertopal_84a8f5d296e74188ac300df24ca140fa/media/image15.png){width="5.430555555555555in"
> height="4.388888888888889in"}

\(b\)

> ![](vertopal_84a8f5d296e74188ac300df24ca140fa/media/image16.png){width="5.652777777777778in"
> height="4.388888888888889in"}

\(c\)

![](vertopal_84a8f5d296e74188ac300df24ca140fa/media/image14.png){width="0.20833333333333334in"
height="0.4305555555555556in"}

> ![](vertopal_84a8f5d296e74188ac300df24ca140fa/media/image17.png){width="5.430555555555555in"
> height="4.388888888888889in"}

\(d\)

> ![](vertopal_84a8f5d296e74188ac300df24ca140fa/media/image18.png){width="5.736111111111111in"
> height="4.388888888888889in"}

+---+-----------------------------------------------------------------+
| β‚¬ |                                                                 |
|   | -- ------------------------------------------------------------ |
|   | --------------------------------------------------------------- |
|   | --------------------------------------------------------------- |
|   | --------------------------------------------------------------- |
|   |      ![](vertopal_84a8f5d296e74188ac300df24ca140fa/media/image1 |
|   | 9.png){width="0.20833333333333334in" height="0.4305555555555556 |
|   | in"}![](vertopal_84a8f5d296e74188ac300df24ca140fa/media/image20 |
|   | .png){width="5.430555555555555in" height="4.402777777777778in"} |
|   |                                                                 |
|   | -- ------------------------------------------------------------ |
|   | --------------------------------------------------------------- |
|   | --------------------------------------------------------------- |
|   | --------------------------------------------------------------- |
|   |                                                                 |
|   | β‚¬                                                               |
+---+-----------------------------------------------------------------+
